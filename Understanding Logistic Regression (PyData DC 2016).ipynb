{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Logistic Regression (PyData DC 2016)\n",
    "#### Nadia Udler, Nicholas Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) is a well-known method to model conditional probabilities. Despite its name, it is a linear classification model rather than regression. To begin with, let's consider a simple example as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Passing Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a group of 20 students who spend between 0 and 6 hours studying for an exam. We'd like to know how the number of hours spent studying affects the probability that the student will pass the exam. We store our sample in `numpy.array`, denote 1 if a student passes the exam and 0 if fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "hours = np.array([0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,\n",
    "                  2.75,3.00,3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50])\n",
    "pass_exam = np.array([0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive way to approach this problem is to use linear regression. But the dependent variable is binary rather than continuous in our case. And although it doesn't hurt to interpret numbers between 0 and 1 as probabilities, for very large and small independent variables, the output of the linear regression can be either negative or larger than 1, making the result nonsensical. Also, for the linear equation to hold, error term has to adjust so that they are correlated with independent variables. \n",
    "\n",
    "\n",
    "However, another argument not to use linear regression is simply because visually, the problem is not linear at all! Try the following code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x8399be0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFRCAYAAABzDARaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x71bfc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Linear regression\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(hours.reshape(-1, 1), pass_exam)\n",
    "\n",
    "# Plot\n",
    "x = np.arange(np.min(hours), np.max(hours), 0.01)\n",
    "y = model.coef_ * x + model.intercept_\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(hours, pass_exam)\n",
    "plt.plot(x, y, 'red')\n",
    "plt.title('Linear Regression')\n",
    "plt.xlabel('hours')\n",
    "plt.ylabel('pass_exam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Conditional Probabilities to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle our problem, we may conceive of a rule that guesses result based on input values. But we definitely need a bridge to jump to a decision of \"pass\" or \"fail\". And it's natural to come up with probabilities as the bridge. In short, we can model the probability of passing (or equivalently, failing the exam) instead and then give predictions based on such probability.\n",
    "\n",
    "<br>\n",
    "Assume that the event of interest is passing the exam, further assume independent observations, then the conditional likelihood function of y given x is\n",
    "\n",
    "<br>\n",
    "$$\\prod_{i=1}^{\\infty} \\Pr\\left(y=y_{i}|x=x_{i} \\right)$$\n",
    "\n",
    "<br>\n",
    "Now, one way to think about each term inside multiplication symbol is that they follow [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) with parameter $p_{i}, i=1,...,n$. Then the above conditional likelihood function becomes\n",
    "\n",
    "<br>\n",
    "$$\\prod_{i=1}^{\\infty} \\Pr\\left(y=y_{i}|x=x_{i} \\right)=\\prod_{i=1}^{\\infty} p_{i}^{y_{i}}(1-p_{i})^{1-y_{i}}$$\n",
    "\n",
    "<br>\n",
    "But maximizing above function by changing $p_{1}, p_{2},..., p_{n}$, we get trivial estimators $\\hat{p_{i}}=1$ if $y_{i}=1$ and $\\hat{p_{i}}=0$ if $y_{i}=0$. The reason such result is useless is becasue that we didn't even us $x$! In fact, it makes sense to believe that close $x$ inputs are more likely to have close outputs. So instead of assuming a sequence of Burnoulli distributions, we assume a parameterized conditional probability as $\\Pr\\left(y=1{\\mid}x\\right)=\\Pr\\left(y=1{\\mid}x;\\beta\\right)=p(x; \\beta)$ where $\\beta$ are parameters in the function. Then the likelihood function becomes\n",
    "\n",
    "<br>\n",
    "$$\\prod_{i=1}^{\\infty} \\Pr\\left(y=y_{i}|x=x_{i} \\right)=\\prod_{i=1}^{\\infty} {p(x_{i};\\beta)}^{y_{i}}(1-{p(x_{i};\\beta)})^{1-y_{i}}$$\n",
    "\n",
    "<br>\n",
    "Now, as long as we know the exact form of function $p(x;\\beta)$, we can explicitly solve the problem using [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation). This last piece requires a bit of imagination and is left for the next part logit function and logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit Function and Logistic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we mentioned that we cannot model probability directly as a linear expression of $x$ because output outside range $(0,1)$ can be obtained. This suggests we modify $p(x;\\beta)$ first to let it cover every real number. Mathematically, we can use the logit function\n",
    "\n",
    "<br>\n",
    "$$f(x)=\\ln\\left(\\frac{x}{1-x}\\right)$$\n",
    "\n",
    "<br>\n",
    "which maps values from interval $(0, 1)$ to ${\\rm I\\!R}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x43c2ef0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x43c2f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot logit function\n",
    "x = np.arange(0.001, 1, 0.001)\n",
    "y = np.log(x / (1 - x))\n",
    "plt.plot(x,y)\n",
    "plt.title('Logit Fucntion')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.text(0.75, 3.5, r'$f(x)=\\ln\\left(\\frac{x}{1-x}\\right)$', horizontalalignment='center', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, a talented reader may wonder why we use logit function when there are other functions that map probabilities to reals? While some may argue that it is because of its simplicity and interpretability (note that we can interprete $\\ln\\left(\\frac{x}{1-x}\\right)$ as logarithm of odds ratio), it's probably more of a matter of personal taste. Remember that [\"All models are wrong but some are useful\"](https://en.wikipedia.org/wiki/All_models_are_wrong), using logit function doesn't guarantee you everthing. See [link](http://stats.stackexchange.com/questions/48072/is-the-logit-function-always-the-best-for-regression-modeling-of-binary-data?newreg=e51a5f412a1549e5b0e3b0c99c0fb549) about a discussion on this question and learn other alternatives to logit function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using logit transformation, we estimate log odds ratio as a linear expression\n",
    "\n",
    "<br>\n",
    "$$\\ln\\left(\\dfrac{p(x;\\beta)}{1-p(x;\\beta)}\\right)=\\beta'x$$\n",
    "\n",
    "<br>\n",
    "where $\\beta$ and $x$ are treated as column vectors. For notational convenience, we assume $\\beta'x$ contains the constant term $\\beta_0 \\cdot 1$. Readers should keep in mind that the right side of the equation should in fact be $\\beta'\\begin{pmatrix}1\\\\x\\end{pmatrix}$, however, this is quite cumbersome.\n",
    "\n",
    "<br>\n",
    "Take exponential function of both sides and after some simple algebraic manipulation, we end up with the exact form of $p(x;\\beta)$ as below\n",
    "\n",
    "<br>\n",
    "$$p(x;\\beta)=\\dfrac{1}{1+e^{-\\beta'x}}$$\n",
    "\n",
    "<br>\n",
    "Mathematically, the function on the right hand side with regard to $\\beta'x$ is called [logistic function](https://en.wikipedia.org/wiki/Logistic_function), also known as sigmoid function or expit function. It can be easily shown that logsitic function is in fact the inverse of logit function. Rather than roll your own, you can use the built-in `expit` function from `scipy.special` module. Let's first have a visual impression of logistic function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x9870780>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAADUCAYAAAC4XdtQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYFNXV+PHvQWRfhkURQZYfIFEURyMIeZFFghCXqFEi\nGsUhCEZFcd8jiMorCooaTZDgHl4UQcW4QmQ0iAsCAwKyKQOyL7I4LIIz5/fHrWGapmeYobu6q2vO\n53nu013Vt6tun66Z23XvrVuiqhhjjDEmeCqkugDGGGOMic0qaWOMMSagrJI2xhhjAsoqaWOMMSag\nrJI2xhhjAsoqaWOMMSagrJI2ZSYiQ0VkU5L2dZWI5ItItVLmbyUiQ0SkVjzbiXrvsyIywXteTUQe\nFJHFIrJLRNaLSLaI/DkR+/KTiLwgIl+VIt8mEbk/xvo7ROQL7/mLIlIQI+WLyG98KPt0EXk9YrmH\niAwu5XuziynnPYkuZynLc7uIdI6xvkBErktFmUxwVUx1AUxaUi8lw7+Bjqq6q5T5jweGAC8AO+LY\nTqRzgHu955OBU4AHgYXA0UBn4HfA8wnYl5+GAVXjeP85wDvecwW+BbIAicq3MI59FOdaYF/E8tnA\nxcCTpXivAh8Dd3NgWX9IWOnK5g7gaeDTqPUdgBXJL44JMqukTaCp6hZgSxneIsT4AXEY23EbEzkJ\naAR8ICItcZXDJao6OSLbxETsy2+qetgVgIjUBn4D3BSxeqeqzoq7YKWgqouji1TGTfyYrLIeLlU9\nZCuHKX+sudv4QkSaichbIrJdRHaIyBQRaRGVJ0NEJohInois9ppTR4rIiog8WV4zYLWIdXeLyDIR\n2e01N78nIkeLSBdgipct13vf9yVsp4qIPCoiuSKyR0S+F5GHoz7KOcAsr+LN8NZtOMRnj7Wv40Tk\nfa+J/DuvSXyiiEyPyDPUa2puLyKzvLz/FZGmInKUiLwpIj+JyCIR6Ra1zwre+1d6n2WBiFwWledF\nEZkVta6ziMzzYjlLRDoW87F6AZtUNaekzx617b+JyEYRqR+x7mIvNr+NitXpIvKp95mXiMiFUdvK\nLmzuFpEhwC1A04jm6+c5TLG+L2/9ChF5NGJ5uvedXeYdf9u9Y+/YqPcVe1x5x3ZdYGhEs3tn77WD\nmrtFZJCILPW2s0xEbop6vfCYyRSRz0Vkp4jMEZFOhxsPEyx2Jm0STkQq4ZoXfwb6A/m4ptZsETlZ\nVbd5WV/CnZ3dgKv4bsE1V/8SsbkDmtZFpC9wF67JcBFQDzgLqA7MBm4DHgMuBNZ7ZThoO54pwBle\n2ebgzpjPjMpzLvCu93wJsBN4UkTuBj5V1Z85WKx9vQPUwjUP/wzcDxwFLI96XzVgDPCot6+ngFe9\n97wHPAPcCbwuIsep6h7vvQ96n30o8DWuKfhfIlKgqq/FKpeINPS2+QWuKfhY4F/EbhI/x8t7ABE5\n4qAPr5rvPb0D1/IwBrhYRI4GngWeVdVpEWUCmOC99jBwtff5fq2q30TlA/gn0ArohvueBTjUGAmJ\nLmtEOcvSfXMG0BB3rFbFfT/PAedF5CnpuLoImI5rffmnt25RMQUe4G1/JPAR7vOOEpFKqlr446Hw\nmHkReAJ3zA8FJolI04jjw6QrVbVkqUwJ1+e7sYTX/wLsBZpGrGuEq2ju9JZPAgqAP0TkqYL7Z/t9\nxLqrcJV8NW/5aWBiCfs+18vfJGp99HZ6evs/t4RtZeD6QTMj1vXB9XUXeJ/nE+DqQ+yrsEynReQ5\n1ovRx1FxzQc6Ray71tvXvRHrTvDW9fSW6wB5wH1R5XgX+DZi+QXgq4jlR714V45Yd7m37fsj1gmw\nEbgwalsFMVJ+VBl+48XwClx//jKgalSsCgqPi4j9fQuMj1g3HXg9YvmxyOPkEMfr9FjlBCrE+r4i\n3rcCeDRqO1uBWhHrBnvvrVyG42pTZHwj1hcA10XEYDXwz6g8z3hlqBR1zHSJyHOKt62z/fofYCl5\nyZq7jR/aAXNUdWXhClVdA3wGFDbDnY47C/h3RJ49wDRKlgOc6zXztRORwz2GuwFbVPXdEvL0AjZo\nRBOvqk4AmgL9gP/DndE9JyL/KmE7pwPrVXVOxHbW4s78o+1V1RkRy8txcZoetQ7cDx9wP3iqAm9E\nbes14HgRqVdMudoBU/XA1oA3Y+Q7A9cKMDVq/SLg17jPV5jaRWZQ1ZnA48BY4HzgKlXdHbUdBd6K\neI8CbwPtiyn34fhPVFnbqWrBYWxnlqpGDkgsPAsu/C5Kc1yVRmPcD7lY32kt4OSIdXtV9ZMYZWoc\nZxlMAFglbfzQkNj9thtw/XEADYCfVHVvVJ5DNVs+j2ua7Y1rpt0g7pKosg4kqgesO0SemE28qrpV\nVV9S1SzgONxZZR8ROTk6r+cYYn+uWOt+iloujE9hFwGqWjjKuYr32NB7jI554XJdYjsGd4a8n1eB\n5kXlOwfXtL8zav0uVZ2rqnMiU4z9TAAqAwu8SjuWjTGWG8bKeJi2Rpf1MLezLWq58Psp/C5Kc1yV\nRkPcj5dY36lw4Hd6wDET4/gwacwqaeOHdbhLk6I1AH70nq8Hanr915GOKmnD6jypqm2AJrhmz7uB\nAWUs4xZKqAS8Sr8XRf3RxZUnH9cXKMCvism2ntifq8TPWgaFlUJ0zBt4jz8S2/ro94hIVaBGVL7I\nfvky8fqBxwLzgTYicnUxWaPLfjSJqexKo7DfNvpYrHMY2yrxuCqDdbhjqqzfqQkZq6SNH74Efi0i\nTQtXiEgjXP/kf71VX+P+Cf0+Ik9VoEdpd6Kqa9QNoFkOnOitjj6zKc5/gLoick4xr3cAahLRxCsi\nNUQk1naPJ/ZZT6FZwDEicnrEthrhml8TYQGwG9e6EOlSYKm6kenFlatH1Gf6Q2QGb+TyqRxmJY27\nvrwVcAGuD3yUiDSJyiO4AVWF+xQvf0mXJO0lcWeKq70ynBBRhsIm/rI61HEFpSv7amAtsb/T7cA3\nB73DhJKN7jaHq7KIXBxjfTZupOmduGuL78cbiIRrwnwOQFUXisg7wD/EzQ62AbgZN6K52L5CEfkH\n7iziC9w/q7OAlrh/juBGYAvwF3GzhO1S1QXR21HVqSLyETBeRB7EjcI9FjhTVf9CURNv5IQkrYEp\n3uU+M4FduArsHmAuENmXHLmv90RkPjDRGxW+x4vH+pI+a+THLulFVd0qIqOB+0Qkn6LR3b1wA92K\nMxq4HnhXRB7H9ave5X2uQucAy1R1eYz3V/cqs2jLVXWLiJyKq6SvV9WVIvIArl/6BaB71HuuFpF9\nuB8cA4AWuAqpOIuBBiJylfeezZFjIMroK2AN8JR3vNYDbscdX6Wx//spxXFVWPZzReRDXNfC4uiu\nBFVVERmK+/v4EfdjsStwDXB3jG4iE1apHrlmKf0SRSNKY6XOXp5muNG823Gjod8GWkRtJwM3+Oon\nXPPefbhKfE5EnuiR0lfhzsY34/7B5QBZUdu9GTcydy/eCODo7XjrKuPO7lbhzkS/A4Z5r80BboxR\n3qHA57j+5DzcIJ3hQEZxZfbWHYfr397lle1q4ENgclRcN0bts4u3rROj1ucD10Ysi/f+lbgfAQuA\nPlHvOWB0t7eusxfD3d5n7oj7MfVX7/XJwOMxjoEXSjgGLgeOBOYB70W97xSvfNdHxep03I+cXbgf\nWhdGvW868FrUdzcO90MnH3i+hOP1gPcWk+fXuBagPNyAvo7A9xw8uvu1qPcd9P0Uc1w9GPH6abgf\neT9x4N/MAd+pt+56YKkXs+UxjsmYV1rE2pal9EzifaFxEZFxuOsEN6hq2xivX447s8I7MK/Vousf\njQH2918uAL5Q1X4pLMexuCkjj1fV73zaRy1cJfCUqg7zYx/xEpEjcT+GLlLVj33ax1W4wYA1NXjT\nqBqTcolq7n4Bd/3qy8W8/j3u1+J2EemFG0jSIUH7NmlKRC7BNQV+A9TGNXO2xF1TmzLqLo86aJKO\neIjINbim7WW4wUC34AYqvZDI/SSSulHCtVNdDmPKs4RU0qo6I3KQUIzXv4hY/IKiawpN+bYTd71x\nC1yl+A1wnqrGun443e3BzcDVFDfI7Eugu6qm6iYPxpg0kJDmbgCvkn4nVnN3VL7bcM2IAxOyY2OM\nMSakkjq6W9xNAfpRNOtUrDyJ+dVgjDHGpAlVjXkVR9KukxaRtriRu79X1a0l5U31aLqypCFDhqS8\nDGFPFmOLc1iSxdhiHCuVJJGVtFDM9Zze5AWTgCvVp9GyxhhjTNgkpLlbRMbjLrSvJyKrcNfuVcJd\nk/8c8FfcXLPPerMJ7VPVRE6enzK5ubmpLkLoWYyTw+LsP4ux/8IW40SN7r78EK8PoOxzK6eFzMzM\nVBch9CzGyWFx9p/F2H9hi3HCRncnioho0MpkjDHG+EVE0FQPHDPGGGNM2VglHafs7OxUFyH0LMbJ\nYXH2n8XYf2GLsVXSxhhjTEBZn7QxxhiTQtYnbYwxxqQhq6TjFLb+jyCyGCeHxdl/FmP/hS3GVkkb\nY4wxAWV90sYYY0wKWZ+0McYYk4asko5T2Po/gshinBwWZ/9ZjP0XthhbJW2MMcYElPVJG2OMMSlk\nfdLGGGNMGrJKOk5h6/8IIotxclic/Wcx9l/YYpyQSlpExonIBhGZX0Kep0RkmYjkiEi4bvhpjDHG\n+CAhfdIi0gnIA15W1bYxXv8dMEhVzxWRM4AnVbVDMduyPmljjDHlhu990qo6A9haQpYLgJe9vF8C\ntUWkQSL2bYwxxoRVsvqkGwE/RCyv8dalvbD1fwSRxTg5LM7+sxj7L2wxDuTAsSwRhnpptAjZIjB0\nKOC+gMgvITsry73upewk58/JyQlUecKYP6dbt0CVx/Jb/sPNn/PII4EqTxjz53TrlrTyfPxxNh9+\nmM327bBxI0zsfR3jpSGL5VfMl7aMkeN5Rk7kk6wXmDYNRozI5uGHs5k+PZuhQ4eSlZlJlsRs5d4v\nYddJi0hT4J1i+qT/AUxX1de85cVAF1XdECOv9UkbY4wps4IC2LULdu6EvLyix8J1kY+7dsHu3UXP\n9+xxy5GPkennn4seC5/n50Plyi5VquRS4fMjjzz4sTC99RYccURRuUvqk66YwPiIl2KZAlwPvCYi\nHYBtsSpoY4wx5dMvv8C2bbB1a1Hatg22by963L4dduw4MP30U1HauROqVIGaNaF6dahRwz0WpmrV\nDk5HHQVVq7rnVaq451WqFD2vXNk9j3wsfF6xojvJ9lNCKmkRGQ90BeqJyCpgCFAJUFV9TlXfE5Fz\nRGQ5sBPol4j9BkF2djZdu3ZNdTFCzWKcHBZn/5WXGKuyvwl4wwb3uGkTbN5c9Lh5M/z4I2zZ4h7z\n8qB2bahTBzIyih4zMtz6jAxo0MA9r1XLpZo1ix5r1HDpv/8NV4wTUkmr6uWlyDMoEfsyxhiTGqqu\nQl2z5sC0bh2sX1/0uGGDa+I9+mhXsR51lHt+1FHQrBm0awf16hWlunVd5VshkKOkUsvm7jbGGAO4\nPtY1ayA3F1ascI+rVrn0ww/usVIlaNwYGjWCY48tejzmGGjY0KUGDVxTsSmdkvqkrZI2xphyZN8+\nVwEvWQLLl7v03XcurVoF9etD8+bujLdZM2jaFJo0cem441yTskksq6R9VF76mFLJYpwcFmf/JTPG\nO3fC4sWwcCEsWgTffuuWV650Z76tW0OrVtCiBbRs6R6bNXMDotJZOh7HyRrdbYwxJskKCtzZ8Pz5\nB6b16+H44+HEE13q29dVzC1bpn9FXJ7YmbQxxqSJ/Hx3Njx7NsyZ41JOjht8dcop0LatSyef7Crj\nyGtxTXBZc7cxxqShDRvg88/hyy9d+vprN0r69NPh17+G006DU091o6NN+vL9BhvlWeQUcsYfFuPk\nsDj7r6QYq7qz5LFjISvL9Rf/6lcwZoxrnr79djfga/lymDDBLXfvbhV0tLAdx9YnbYwxKaAKS5fC\n9OmQne1S5crQuTN06gS33eb6ku3a4fLNmruNMSZJNm2CadNg6lSXAM46C7p1g65d3ehqU/5Yn7Qx\nxqRAQYHrR373XZeWLYMuXeDss6FHDzf62u+5n03wWZ+0j8LW/xFEFuPksDgnxq5d7i5H/fq52bey\nsty6kSPhjTeymTIFBg1yl0NZBZ14YTuOrU/aGGPitHUrvPMOvPkmfPyxG3194YVw//1u9q5CIas/\nTBJYc7cxxhyG7dvh7bfhtddgxgzXt3zRRXDeeTbi2pSN9UkbY0wC7Nnjzpj/9S93xtytG1x6KZx/\nvrtdojGHw/qkfRS2/o8gshgnh8U5toIC+PRTGDDAzXn9j3/ABRe4u0K9/TZcfnnpK2iLsf/CFuOE\n9EmLSC9gNK7SH6eqI6JerwW8CjQBjgBGqeqLidi3Mcb4Yc0aePFFeP55qFYNrrzSzYnduHGqS2bK\nk7ibu0WkArAU6A6sBWYBfVR1cUSeu4Faqnq3iNQHlgANVPWXGNuz5m5jTEr88otrzv7nP910nH/8\nI/Tv7waC2Uhs4xe/74LVHlimqiu9nU0ALgAWR+RRoLBBqCawJVYFbYwxqbB2rZuOc+xYN6HIwIEw\ncaI7gzYmlRLRJ90I+CFiebW3LtLfgBNFZC0wDxicgP0GQtj6P4LIYpwc5S3Oqq6v+ZJL4KST3M0s\n3n/fjdTu29efCrq8xTgVwhbjZF0n3ROYq6pniUgLYKqItFXVvFiZs7KyaObNj5eRkUFmZub+m3gX\nfgFBWc7JyQlUecK4nJOTE6jy2HJ6L+/dC+vWdeXJJ2Hz5mz+8AdYubIrNWu617Oz7f9FOi+nw/+L\nwue5ubkcSiL6pDsAQ1W1l7d8F6CRg8dE5N/A/6rqZ97yf4A7VfXrGNuzPmljTMJt2QLPPutS27Zw\n003Qs6fdwMKknt+XYM0CWopIUxGpBPQBpkTlWQn81itMA+B44PsE7NsYY0q0ciUMHgwtW7pbPU6d\nCh9+CL/7nVXQ5V1eXh69e/dm9erVqS5KseI+RFU1HxgEfAQsBCao6rcico2IDPSyPQT8RkTmA1OB\nO1T1x3j3HQSRzRfGHxbj5AhbnBctcpdNnXoqVKoECxa4y6lOOil1ZQpSjH/5JZxjd0sb43HjxjFq\n1CgmT55MQUGBv4WKQ0L6pFX1A6B11LoxEc/X4fqljTHGV/PmwUMPuUFhgwfD009DRkaqSxUsb7zx\nBnl5eWRlZZX5vUOHDuXCCy8kMzMz8QVLov79+wPwwAMPpLgkJbNpQY0xoTBnDgwbBl99BbfeCn/5\nC1SvnupSpc7y5cu56aabaNOmDTt27ODvf/87AB9//DFTpkxh9OjRh7Xdn3/+mfPPP58xY8bQPPLu\nIWmqQoUK5Obm0qRJk5SVwaYFNcaE1oIF8Ic/uPmzu3eH775zlXR5rqD37dtHr169uOSSS1i/fj3j\nxo1j+/bt7Nixg7vuuotHHnnksLdduXJlnn32Wa688krshMp/VknHKUh9TGFlMU6OdIvz0qVu3uzf\n/hbOPBOWL4cbboCqVVNdsuIlK8YffPABK1asoEuXLtxwww28//771K5dm+HDh3PFFVdQpUqVuLbf\nsmVLmjRpwvjx4xNU4sRJt+P4UOx+0saYtLJmDTzwgLt38y23wHPPQY0aqS5VsHzyySfUr1+f5s2b\n72+S3rVrF2PHjmX58uUJ2cfgwYP585//zJ/+9KeEbM/EZn3Sxpi0sG0bjBjhKuUBA+DOO6FOnVSX\nKpg6d+5MrVq1+Pe//71/3cSJExkxYgRff33Q9BSHpaCggLp16/LZZ5/Rpk2bhGwzFYLeJ21n0saY\nQPv5Z3jmGXjkEfj9793obbsTVWxZWVls2LCBGTNmcMIJJ3DOOefQvHlznnnmGaZOncpvfvObYt87\nZ84cXn31VUSElStXMnbsWMaMGcO2bdtYs2YNw4YNO2CgWIUKFejYsSMffPBBWlbS48ePZ8aMGYgI\nd911F506deK6665LdbEOYmfSccrOzt4/5Zvxh8U4OYIWZ1WYPBnuuANOOAEefRROPDHVpYpPMmK8\nYsUKWrRowZtvvskFF1ywf327du0YOHAgAwYMOOg9y5cv5+mnn+bJJ58EoF+/fsycOZOXXnqJgoIC\nzjzzTEaOHMnNN998wPtuu+02NmzYwCuvvFJsefr378+cOXOQUtxGTFUREUaPHk3nzp1L+5EPELTj\nuDTsTNoYk1ZmzXL9zTt2wJgxbnCYKZ25c+ciIpxyyikHrM/NzSWjmAvGR48ezWOPPbZ/eefOndSt\nW5cOHTqwevVqbr311pjXVNepU4eZM2eWWJ5x48aV/UOY/Wx0d5zS7RdbOrIYJ0cQ4rx+PWRlwQUX\nuMc5c8JVQScjxvPmzaNWrVr7b1JUaPv27cVW0nfeeSdVI4bFz5w5k996gW/cuDGPPvoodWIMAKhb\nty7bt29PXOETIAjHcSJZJW2MSbm9e+Gxx9yUnUcfDYsXQ//+cMQRqS5Z+snJyYk5G5iIFDv95XHH\nHbf/+eLFi1m7di3dunU75L4qVKhAfn7+4RfWHJI1d8cpHfs/0o3FODlSFecPPnDTd7ZqBTNnwvHH\nJ70ISZOMGM+bN4+LLrrooPUZGRn8+OOhb5kwbdo0KleufMAgsxUrVsScXWzLli3Url27xO0NHDhw\nfxP8oRT2SY8aNYozzzzzgNcqVKhQqm0EReFnifdHjFXSxpiUWLXK3S5y/nx46ik455xUlyj9bd26\nlVWrVh3UHw3QvHnzmJX0nj17GDJkCH379qVNmzZMmzaNtm3b7p/wRFUZOXIkzzzzzEHv3bJlyyGn\nBn3uuecO89McqLQ3wQjbj3pr7o5TmA6GoLIYJ0ey4rx3L/zv/8Jpp7k7VC1YUH4qaL9jXNygMYBO\nnTqxaNGig9a/9957jBw5koULF7JkyRK+//57KleuvP/1hx9+mL59+8bc39KlSzn11FMT9wESIGz/\nL6ySNsYkzfTp0Lata9b+6iv4618hzhkqTYQ5c+ZQu3btmJV0r169+OSTTw5a36VLF/r168fs2bN5\n/vnn+fLLL2nRogXXXnstgwcPpmPHjpxxxhkHvU9VmTFjxv4BZskStHtA79ixg3fffZeRI0fy0ksv\n0bdvX/Ly8hK2fbtOOk5ha1oJIotxcvgZ502b4LbbIDvbNW1HXL5brvh9LF9++eXs27ePiRMnHvTa\n3r17adSoEfPnz6dhw4Zx72vWrFlcccUVLFmyJO5tlda4ceNYvXo1w4YNY8WKFTFnCYuO8aRJk7j4\n4osPe595eXlMmjTpoP7whg0b0qNHD1atWkWTJk0YPnw4Z599Nq1bt6ZmzZpl2ofvd8ESkV4islhE\nlorIncXk6Soic0VkgYhMT8R+jTHBVlAA48a5UdtHHQULF5bfCtovI0aMoGfPnoCrOHv37h0zX6VK\nlbj++usP+xaV0Z5++mluuummhGyrtPr378+QIUPKdPetBQsWxLXPGjVqcNVVV9G3b98DUo8ePQB3\nrXh+fj6ff/45p59+Olu3bo1rf9HiPpMWkQrAUqA7sBaYBfRR1cUReWoDM4GzVXWNiNRX1c3FbC+t\nzqSNMbEtXgzXXAN79rgJSWJcFWQS4OSTT6ZZs2YMHz6cPn36MH/+fI4o5tq1nTt30rFjRz799NNi\nr5kujRUrVnDRRRcxe/bsYvflp7LMtz1s2DDuv//+Q+ZTVZ544gkqVqxI7dq1+fHHHw+aYS2W++67\nj1atWjF37lzOP/98qlevTocOHUr1OQr5fSbdHlimqitVdR8wAYj+rXw5MElV1wAUV0EbY9Lf3r3w\n4IPQqRNcconrf7YK2j+33347jRo1Yvjw4UyaNKnESrN69eqMHTuWq6+++rD398svv3Ddddfxyiuv\npKSCLqvSnvRdc8015Ofnc+ONN9K7d282by5dNfXQQw9x1VVXMXr0aLp3717mCvpQEnEmfTHQU1UH\nestXAO1V9caIPE8ARwJtgBrAU6oac7LXdDuTtv5S/1mMkyMRcf78c3eHqubN4dlnIWKODENwjuUP\nP/yQxYsXM3jw4DK/d8iQIXTr1i2ln6O4M+mNGzdy880307RpU8BV0J999hmdOnXaX1nXrFmTe+65\n54D3LVmyhFNPPZUxY8ZQoUIF9uzZQ+/evalVq1ZSPk8Q5u6uCJwGnAVUBz4Xkc9VNeaNTbOysvZP\naZeRkUFmZub+A6Lwht5BWc7JyQlUecK4nJOTE6jy2PLBy6ef3pV774VXXsnmhhtg6NCuiASnfEFZ\nDsr/i549e9KzZ8/Den+XLl1SXv5C0a8vWrSIdu3a7e8rz87OZv369QwfPrzE92/cuJGTTjpp/8xr\nySh/dnY2ubm5HEoizqQ7AENVtZe3fBegqjoiIs+dQBVVfcBb/ifwvqpOirG9tDqTNqa8mzoVBg6E\nzp3h8cehXr1Ul8iEXVn6pB944AGGDBlSYp6cnBxuvPFGPv300/3rXnzxxZg3FfGD32fSs4CWItIU\nWAf0AS6LyvM28LSIHAFUBs4AHk/Avo0xKbJ1K9x6K/znP25gWK9eqS6RCbvDuQd0aaYSzczM5OKL\nL+app56iXr167N69m3PPPTdRxY5LQq6TFpFewJO4gWjjVPUREbkGd0b9nJfnNqAfkA+MVdWni9lW\nWp1JZwekjynMLMbJUZY4T5kC114LF14IjzwCZbwstNyyY9l/0TF+7bXXuPTSS1NXoFLwvU9aVT8A\nWketGxO1PBIYmYj9GWNSY/NmuPFGN1vY+PHQpUuqS2RMyYJeQR+KzThmjCmVN96AG26Ayy6Dhx6C\natVSXSJjwiEIo7uNMWlq0ya4/nqYNw8mTYKIOxgaY3xmN9iIU/SQfpN4FuPkiBXniRPh5JOhaVPI\nybEKOl52LPsvbDG2M2ljzEEKz57nz4c334SOHVNdImPKJ+uTNsYcYOJE1/fcty888ABUrZrqEhkT\nbtYnbYw5pE2bYNAg1/f81luQ4CmIjTGHwfqk4xS2/o8gshj7b/JkaN06m+OOg7lzrYL2ix3L/gtb\njO1M2phybPNm17Q9e7Zr2r7hhlSXyBgTyfqkjSmn3nzTDQ7r08euezYmlaxP2hiz35YtMHgwfPkl\nvP66u++g7jrHAAAQGUlEQVSzMSaYrE86TmHr/wgii3HivPWWu+65fn03QCyygrY4+89i7L+wxdjO\npI0pB7Zscf3Ns2bBa6/BmWemukTGmNKwPmljQq6w7/nSS+Hhh63v2ZigsT5pY8qhwuue58yxs2dj\n0pX1SccpbP0fQWQxLhtVVymffDI0aeL6nktTQVuc/Wcx9l/YYpyQM2kR6QWMxlX641R1RDH52gEz\ngUtVdXIi9m2MKbJ+PVx3HSxeDG+/DWeckeoSGWPiEXeftIhUAJYC3YG1wCygj6oujpFvKrAbeL64\nStr6pI0pO1V46SW44w4YMADuvx8qV051qYwxpeF3n3R7YJmqrvR2NgG4AFgcle8G4A2gXQL2aYzx\n5ObCwIFu9rCPPoLMzFSXyBiTKInok24E/BCxvNpbt5+IHAtcqKp/B2L+WkhXYev/CCKLcWz5+fDU\nU9CuHXTvDl99FV8FbXH2n8XYf2GLcbJGd48G7oxYLrGizsrKolmzZgBkZGSQmZlJ165dgaIvICjL\nOTk5gSpPGJdzcnICVZ4gLNer15UBA2D37mxGjYK+fYNVPluOvWz/L/xfTof/F4XPc3NzOZRE9El3\nAIaqai9v+S5AIwePicj3hU+B+sBOYKCqTomxPeuTNqYYe/bAgw/C2LHumuf+/aGCXaNhTFrzu096\nFtBSRJoC64A+wGWRGVT1/0UU5gXgnVgVtDGmeB9/DH/5C5xyirusqmHDVJfIGOO3uH+Dq2o+MAj4\nCFgITFDVb0XkGhEZGOst8e4zSCKbL4w/ynuMN26EK6+Efv1g1CiYONGfCrq8xzkZLMb+C1uME9In\nraofAK2j1o0pJu+fE7FPY8KuoACefx7uuQeuugoWLoQaNVJdKmNMMtnc3cYEUE6Om287Px/GjHFN\n3MaYcCqpT9qGnBgTINu3u3s99+wJWVkwc6ZV0MaUZ1ZJxyls/R9BVB5irAqvvgonnAC7d8OiRW7m\nsGSO3C4PcU41i7H/whZjuwuWMSk2Zw7ceKO7vOrNN22+bWNMEeuTNiZFNm6Ee++Fd95x1zz362fX\nPBtTHlmftDEBsncvPPEEtGkDNWu6O1bZpCTGmFjs30Kcwtb/EURhibEqTJ4MJ54I//kPfPopPP44\nZGSkumROWOIcZBZj/4UtxtYnbUwSfP013HILbNsGf/879OiR6hIZY9KB9Ukb46PvvoP77oNPPoFh\nw1y/8xFHpLpUxpggsT5pY5JswwYYNMiN1D7pJFi2DK6+2ipoY0zZWCUdp7D1fwRROsV42zb4619d\nv/ORR7pBYffeC9Wrp7pkh5ZOcU5XFmP/hS3GVkkbkwA//QQPPQStWsGaNTB7thvBXb9+qktmjEln\n1idtTBzy8txAsJEj4be/hSFD4PjjU10qY0w68ft+0saUOzt2wDPPwOjR0KWLu6TqpJNSXSpjTNhY\nc3ecwtb/EURBivHWrW6UdosW7taR06fD66+Ho4IOUpzDymLsv7DFOCGVtIj0EpHFIrJURO6M8frl\nIjLPSzNE5ORE7NeYZPnhB3edc4sWsGKFuzvVq6+6AWLGGOOXuPukRaQCsBToDqwFZgF9VHVxRJ4O\nwLequl1EegFDVbVDMduzPmkTGAsWwGOPufm1+/WDm2+Gxo1TXSpjTJj4fZ10e2CZqq5U1X3ABOCC\nyAyq+oWqbvcWvwAaJWC/xviioADeew/OPtvNDNa6tZuUZNQoq6CNMcmViEq6EfBDxPJqSq6Erwbe\nT8B+AyFs/R9BlKwY5+XBs8+6ezrfdx9ccQXk5sI990CdOkkpQkrZsew/i7H/whbjpI7uFpFuQD+g\nU0n5srKyaNasGQAZGRlkZmbStWtXoOgLCMpyTk5OoMoTxuWcnBxft79iBcye3ZX/+z9o0yab66+H\nG27oikgwPr8th2fZ/l/4v+z3/4tELBc+z83N5VAS0SfdAdfH3MtbvgtQVR0Rla8tMAnoparflbA9\n65M2vtuzB956y13jvGwZDBjgkjVnG2OSze/rpGcBLUWkKbAO6ANcFlWAJrgK+sqSKmhj/DZvHowb\nB+PHQ2amm1/7wgvdFJ7GGBM0cfdJq2o+MAj4CFgITFDVb0XkGhEZ6GX7K1AXeFZE5orIV/HuNygi\nmy+MP+KN8caN8PTTcPrpcN557v7Ns2bBtGnQu7dV0IXsWPafxdh/YYtxQvqkVfUDoHXUujERzwcA\nAxKxL2NKY/dumDIFXnkFZsxwlfPDD7upO+1OVMaYdGFzd5vQ2LMHPvzQzQD27rvQvj1ceSVcdBHU\nqJHq0hljTGwl9UlbJW3S2q5d8NFHMHmym3AkMxP++Ee4+GI4+uhUl84YYw7N78lMyrWw9X8EUXSM\nt2yBl192Z8gNG7r+5vbtYdEiN5f2tddaBX047Fj2n8XYf2GLsd0FywSeqhuV/e67Ln3zDXTv7irp\nceOgbt1Ul9AYY/xhzd0mkDZudLd//OgjmDoVqlSBc891qXNnt2yMMWFgfdIm8LZvd6Owp093lfOK\nFdC1q5s7u0cPaNUKJOYhbIwx6c36pH0Utv6PZNm0yc34ddtt0K4dNGoEjz8OtWvD3/5W9Pr118Pa\ntdlWQSeBHcv+sxj7L2wxtj5p47tffoGFC+HLL+Hzz+Gzz1xzdocO8D//4yrn9u2hcuVUl9QYY4LF\nmrtNQhUUuLmw58yB2bPhq69g7lw3J3b79kUVc5s2NqmIMcaA9Ukbn+zcCQsWwPz5LuXkuHTUUXDa\naS61b++m48zISHVpjTEmmKyS9lF2dvb+25CF1c6dsGSJuw554cKixzVr3L2X27YtSqedlvhLospD\njIPA4uw/i7H/0jHGft8Fy4TAnj1uRPXy5fDdd7B0qauYlyxxk4e0bOmaqE88Efr2dZVzq1Z2cwpj\njPGTnUmXEz//7M58V62C3FyXVqxwj99/70ZTN2kCLVq4dPzx0Lq1S8cdZ/3HxhjjF2vuDjFV2LoV\n1q+Hdetg7VpXGRem1avhhx/gxx/h2GNdhdu0KTRvDs2aFT0edxxUtHYVY4xJOqukfZTo/g9V1we8\nZQts3uzOcDdvdmnjRtiwoehxwwZXOVet6uawPuYYVxE3alSUGjd2FfAxx6Tv2XA69jGlI4uz/yzG\n/kvHGPveJy0ivYDRuMlRxqnqiBh5ngJ+B+wEslQ1JxH7DhpV17+7Y0dR2r7dpW3bih63bXNnwJFp\nyxZ3xluxItSrB/Xru5HS9esXPe/QARo0cDeQaNDAVb5Vq6b6UxtjjPFD3GfSIlIBWAp0B9YCs4A+\nqro4Is/vgEGqeq6InAE8qaoditmer2fS+fmuf3bPHti926XI57t2Hfi4c6d7XviYl+ee5+UdmH76\nqeixYkWoWRNq1SpKGRluNq3Cxzp1Dk716rmR0TYvtTHGlB9+n0m3B5ap6kpvZxOAC4DFEXkuAF4G\nUNUvRaS2iDRQ1Q2xNjhyJOzb52aq2rcP9u51j5HP9+49MP38c9FjcWnPHldJV63qZreqWtVViIWP\n1aq559WqFT2vXr1ouU4dqFHDratRo+h5zZpFqUYNqFQpAVE1xhhT7iWikm4E/BCxvBpXcZeUZ423\nLmYlvW6du7SnMFWrduBypUpF6cgjXYVbqVLRY6VKrtKtXLkoVaniUsWKib1RQ3Z2Nh07dk3cBs1B\n0rGPKR1ZnP1nMfZf2GIcyBtsbHlcqDJCOOIhodYQocPdwq0/DeXGG+GEE7Jp0SKbK6+ESy+FulOy\nqH6e0ONsoXMXYU9HYcevhRNfH0qLFrB8eTYLF2ZTs6ar0D/pl0W2iKupRcj2EkOHAu4LjpygPTur\n5Pw5OTllyl/W7Vv+LHK6dQtUeSy/5T/c/DmPPBKo8oQxf063boEqT6z82dnZDB06lKzMTLKk5LPG\nRPRJdwCGqmovb/kuQCMHj4nIP4Dpqvqat7wY6BKruTvdRncbY4wx8fD7VpWzgJYi0lREKgF9gClR\neaYAfb3CdAC2FdcfbYwxxhgn7kpaVfOBQcBHwEJggqp+KyLXiMhAL897wAoRWQ6MAa6Ld79BEdnU\nYfxhMU4Oi7P/LMb+C1uME3KdtKp+ALSOWjcmanlQIvZljDHGlBc245gxxhiTQn73SRtjjDHGB1ZJ\nxyls/R9BZDFODouz/yzG/gtbjK2SNsYYYwLK+qSNMcaYFLI+aWOMMSYNWSUdp7D1fwSRxTg5LM7+\nsxj7L2wxtkraGGOMCSjrkzbGGGNSyPqkjTHGmDRklXScwtb/EUQW4+SwOPvPYuy/sMXYKmljjDEm\noKxP2hhjjEkh65M2xhhj0pBV0nEKW/9HEFmMk8Pi7D+Lsf/CFuO4KmkRqSMiH4nIEhH5UERqx8jT\nWEQ+FpGFIvKNiNwYzz6DJicnJ9VFCD2LcXJYnP1nMfZf2GIc75n0XcA0VW0NfAzcHSPPL8AtqtoG\n6AhcLyK/inO/gbFt27ZUFyH0LMbJYXH2n8XYf2GLcbyV9AXAS97zl4ALozOo6npVzfGe5wHfAo3i\n3K8xxhgTevFW0ker6gZwlTFwdEmZRaQZkAl8Ged+AyM3NzfVRQg9i3FyWJz9ZzH2X9hifMhLsERk\nKtAgchWgwH3Ai6paNyLvFlWtV8x2agDZwIOq+nYJ+7Prr4wxxpQrxV2CVbEUb+xR3GsiskFEGqjq\nBhE5BthYTL6KwBvAKyVV0CUV1BhjjClv4m3ungJkec+vAoqrgJ8HFqnqk3HuzxhjjCk34ppxTETq\nAq8DxwErgT+q6jYRaQiMVdXzROR/gE+Bb3DN5Arco6ofxF16Y4wxJsQCNy2oMcYYYxybcSyBRORW\nESnwWhhMAonIoyLyrYjkiMgkEamV6jKFhYj0EpHFIrJURO5MdXnCKOyTOgWJiFQQkTkiMiXVZUkE\nq6QTREQaAz1wzf4m8T4C2qhqJrCM2BPnmDISkQrA34CeQBvgsjBNNhQgoZ7UKWAGA4tSXYhEsUo6\ncZ4Abk91IcJKVaepaoG3+AXQOJXlCZH2wDJVXamq+4AJuEmKTALZpE7J4Z0snQP8M9VlSRSrpBNA\nRH4P/KCq36S6LOXEn4H3U12IkGgE/BCxvBqrPHwVxkmdAqTwZCk0g60OeZ20cQ4xqcs9uKbuyNdM\nGZUQ43tV9R0vz73APlUdn4IiGhMXb1KnN4DB3hm1SRARORfYoKo5ItKVkPwftkq6lIqb1EVETgKa\nAfNERHDNsLNFpL2qxpzcxcRW0sQ5ACKShWvKOispBSof1gBNIpYbe+tMgpVlUidzWP4H+L2InANU\nBWqKyMuq2jfF5YqLXYKVYCKyAjhNVbemuixhIiK9gFFAZ1XdkuryhIWIHAEsAboD64CvgMtU9duU\nFiyERORlYLOq3pLqsoSdiHQBblXV36e6LPGyPunEU0LSzBIwTwM1gKne5RXPprpAYaCq+cAg3Oj5\nhcAEq6ATz5vU6U/AWSIy1zuGe6W6XCb47EzaGGOMCSg7kzbGGGMCyippY4wxJqCskjbGGGMCyipp\nY4wxJqCskjbGGGMCyippY4wxJqCskjbGGGMC6v8DAX3Gi5CnJzUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x97e9208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import built-in logistic function\n",
    "from scipy.special import expit\n",
    "x = np.arange(-5, 5, 0.1)\n",
    "y = expit(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(x, y)\n",
    "plt.hlines([0, 1], -5, 5, colors='r', linestyles='dashed')\n",
    "plt.grid(True)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "plt.title('Logistic/Sigmoid/Expit Function', fontsize=15)\n",
    "plt.text(3.6, 0.65, r'$f(x)=\\frac{1}{1+e^{-x}}$', horizontalalignment='center', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that conditional likelihood function is\n",
    "\n",
    "<br>\n",
    "$$\\prod_{i=1}^{\\infty} \\Pr\\left(y=y_{i}|x=x_{i} \\right)=\\prod_{i=1}^{\\infty} {p(x_{i};\\beta)}^{y_{i}}(1-{p(x_{i};\\beta)})^{1-y_{i}}$$\n",
    "\n",
    "<br>\n",
    "Now plug in the explicit form $p(x;\\beta)=\\dfrac{1}{1+e^{-\\beta'x}}$, take log of both sides and simplify our log likelihood function as much as possible\n",
    "\n",
    "<br>\n",
    "$$\\begin{align}\\ln(L(x, y; \\beta))\n",
    "&=\\sum\\limits_{i=1}^n [y_i\\ln(p(x_i;\\beta))+(1-y_i)\\ln(1-p(x_i;\\beta))]\n",
    "\\\\&=\\sum\\limits_{i=1}^n [y_i\\ln(\\dfrac{1}{1+e^{-\\beta'x_i}})+(1-y_i)\\ln(1-\\dfrac{1}{1+e^{-\\beta'x_i}})] \n",
    "\\\\&=\\sum\\limits_{i=1}^n [y_i\\ln(\\dfrac{1}{1+e^{-\\beta'x_i}})+(1-y_i)\\ln(\\dfrac{e^{-\\beta'x_i}}{1+e^{-\\beta'x_i}})]\n",
    "\\\\&=\\sum\\limits_{i=1}^n [y_i\\ln(\\dfrac{1}{1+e^{-\\beta'x_i}})+(1-y_i)\\ln(\\dfrac{1}{1+e^{\\,\\beta'x_i}})]\n",
    "\\\\&=-\\sum\\limits_{i=1}^n [y_i\\ln(1+e^{-\\beta'x_i})+(1-y_i)\\ln(1+e^{\\,\\beta'x_i})]\n",
    "\\\\&=-\\sum\\limits_{i=1}^n\\begin{cases}\n",
    "    \\ln(1+e^{-\\beta'x_i})       & \\quad \\text{if } y_i \\text{ is 1}\\\\\n",
    "    \\ln(1+e^{\\,\\beta'x_i})  & \\quad \\text{if } y_i \\text{ is 0}\\\\\n",
    "  \\end{cases}\n",
    "\\\\&=-\\sum\\limits_{i=1}^n\\ln(1+e^{\\,-y_i\\beta'x_i})\\qquad\\,\\text{redefine } y_i \\text{ such that } y_i=1 \\text{ if } y_i==1 \\text{ and } y_i=-1 \\text{ if } y_i==0\n",
    "\\end{align}$$\n",
    "\n",
    "<br>\n",
    "We will solve for $\\hat{\\beta}$ such that $\\ln(L(x, y; \\beta))$ is maximized at $\\hat{\\beta}$. The reason that we maximize the log likelihood function instead of the likelihood function is because summing comes easier than multiplication, more importantly, the likelihood function can be very small for large dataset and result in floating point underflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Log (Conditional) Likelihood Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to take advantage of matrix algebra in our code and ensure high performance. The standard way is to use `array` in `numpy` module rather than `list` object in standard Python library. Also, methods in `numpy` module usually support faster vectorized implementation. \n",
    "\n",
    "<br>\n",
    "Suppose $x$ is a 2-dimensional array such that its columns are independent variables and each row is an observation, we further assume that all elements are 1 in the first column of $x$ (don't forget the constant term!). Next, suppose (redefined) $y$ and $\\beta$ are both stored in a column vector. Then, matrix product $x\\beta$ is just a column vector such that its $i$th row is exactly $\\beta'x_i$, and element-wise product $y$ times $x\\beta$ is just a column vector such that its $i$th row is exactly $y_i\\beta'x_i$. The log likelihood function $-\\sum\\limits_{i=1}^n\\ln(1+e^{\\,-y_i\\beta'x_i})$ then is just a one-line code: `-np.sum(np.log(1 + np.exp(- y * np.dot(x, p))))`.\n",
    "\n",
    "<br>\n",
    "One thing to be cautious here is that `np.array` can be 1-dimensional when storing a series of numbers and cause trouble to matrix algbra, especially matrix multiplication. To solve this problem, always use [`reshape`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) method before any matrix manipulations.\n",
    "\n",
    "<br>\n",
    "In the following snippet, a closure is used to separate parameter $\\beta$ and non-parametric variables $x$ and $y$. A closure is essentially an inner function (`log_likelihood_p`) that is dynamically generated by another function and can remember the variables passed from outer function (`log_likelihood`). Also, you can use [`functools.partial`](https://docs.python.org/3.7/library/functools.html#functools.partial) to accomplish the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(x, y):\n",
    "    \"Outer function\"\n",
    "    def log_likelihood_p(p):\n",
    "        \"Inner Function\"\n",
    "        p = p.reshape(-1, 1)\n",
    "        return -np.sum(np.log(1 + np.exp(- y * np.dot(x, p))))\n",
    "    return log_likelihood_p # log_likelihood_p has variable p to be estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Optimization Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to do next is to estimate parameters $\\beta$ in function `log_likelihood_p`. Python's  `scipy.optimize` package provides several commonly used optimization algorithms. Here, we use [basin-hopping](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.basinhopping.html#scipy.optimize.basinhopping) algorithm to find global maximum and choose [Nelder-Mead](http://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html#optimize-minimize-neldermead) solver as its inner method. Readers can also see [here](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) to find other optimization methods. For example, if gradient and Hessian matrix of our log likelihood function are provided, some faster methods may be applied (however, they might suffer from numerical instability). I also tried writing another function to return gradient of our likelihood function and fed it to [`BFGS`](http://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html#optimize-minimize-bfgs) solver in `scipy`, and it worked much faster. But since this tutorial is not about optimization methods, we will leave derivative-based methods to curious readers. \n",
    "\n",
    "<br>\n",
    "But before we run solver, don't forget to redefine $y$. Recall that user inputs $y_i$ such that $y_i \\in \\{1, 0\\}$, but in our `log_likelihood` function, we redefine $y_i$ such that $y_i \\in \\{1, -1\\}$. To transform $y$, we can use the function [`np.place`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.place.html), which changes elements of an array based on conditional and input values. In addition, because solvers in `scipy` by default minimize function, we also need to first negate our log likelihood function, which is done through function `negate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import basinhopping\n",
    "\n",
    "def negate(f):  \n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def logistic_regression(x, y, x0=None, niter=500):\n",
    "    \"\"\"\n",
    "    x0: inital guess to be passed into optimization\n",
    "    niter: the number of basin hopping iterations\n",
    "    \"\"\"\n",
    "    # If x0 not provided, use zero vector\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros_like(x[0])\n",
    "    # Ensure y is 2-dimensional column vector\n",
    "    y = y.reshape(-1, 1)\n",
    "    # Redefine y\n",
    "    np.place(y, y==0, [-1])\n",
    "    minimizer_kwargs = {'method':'nelder-mead'}\n",
    "    result = basinhopping(negate(log_likelihood(x, y)), x0, \n",
    "                          minimizer_kwargs=minimizer_kwargs, niter=niter)\n",
    "    return result.x # Attribute x returns solution array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put Into Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying our program, let's add one more function that checks user input. We ask users to provide `np.ndarray` type data because all the operations inside our log likelihood function is built upon `np.ndarray`. In fact, it's pretty easy to transform from other array-like datatype to `np.ndarry`, simply use `np.array` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_check(x, y, x0):\n",
    "    # For convenience, x, y, initial guess x0 must be np.ndarray\n",
    "    if type(x) == type(y) == type(x0) == np.ndarray:\n",
    "        # check if 1st column of x is a zero vector\n",
    "        if (x[:,0:1] == np.ones_like(x[:,0:1])).all():\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Intercept column is missing')\n",
    "    else:\n",
    "        raise TypeError('np.ndarray is expected but recieved other types')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, be wary of failed optimization. Many reasons can result in a failed optimization. Fortunately, whether or not the optimizer exited successfully is stored inside the optimization result and we can return a warning message when the optimization fails. Now we can modify function `logistic_regression` by incorporating `input_check` and altering output part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(x, y, x0=None, niter=500):\n",
    "    \"\"\"\n",
    "    x0: inital guess to be passed into optimization\n",
    "    niter: the number of basin hopping iterations\n",
    "    \"\"\"\n",
    "    # If x0 not provided, use zero vector\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros_like(x[0])\n",
    "    # Check inputs\n",
    "    input_check(x, y, x0)\n",
    "    # Ensure y is 2-dimensional column vector\n",
    "    y = y.reshape(-1, 1)\n",
    "    # Redefine y\n",
    "    np.place(y, y==0, [-1])\n",
    "    minimizer_kwargs = {'method':'nelder-mead'}\n",
    "    result = basinhopping(negate(log_likelihood(x, y)), x0, \n",
    "                          minimizer_kwargs=minimizer_kwargs, niter=niter)\n",
    "    # Check if optimizer exited successfully\n",
    "    if str(result).find('success: True') != -1:\n",
    "        print('Estimated coefficients: ')\n",
    "        print(*result.x, sep=', ') \n",
    "        return result.x\n",
    "    else:\n",
    "        print('Error: optimization failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Example: Passing Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our `logistic_regression` to our example. It works well and yields estimated coefficients as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated coefficients: \n",
      "-4.07772242367, 1.50464857319\n"
     ]
    }
   ],
   "source": [
    "hours = np.array([0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,\n",
    "                  2.75,3.00,3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50])\n",
    "# Transform to a 2-dimensional matrix, 1st column is a column of 1s\n",
    "hours = np.concatenate((np.ones((20,1)), hours.reshape(-1,1)), axis=1)\n",
    "pass_exam = np.array([0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1])\n",
    "est = logistic_regression(hours, pass_exam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These estimates are entered in the logistic regression equation to estimate the probability of passing the exam. For example, for a student who studies 4 hours, the estimated probability of passing the exam would be $\\dfrac{1}{1+e^{-(-4.0777+1.5046\\times4)}}=0.87$. Next, we define the classification rule that a student is classified to \"pass\" if his/her estimated probability of passing the exam is larger than $0.5$, otherwise, the student is classified to \"fail\". \n",
    "\n",
    "<Br>\n",
    "In general, if $p(x;\\beta)=\\dfrac{1}{1+e^{-\\beta'x}}>0.5$, or equivalently, $\\beta'x>0$, we predict y=1 and classify the event to corresponding side. If on the other hand $p(x;\\beta)=\\dfrac{1}{1+e^{-\\beta'x}}\\leqslant0.5$, or equivalently, $\\beta'x\\leqslant0$, we classify the event to y=0. As mentioned before, logistic regression is a classification method (also note that it is a linear classification method!). In machine learning, it belongs to the category of [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning), a method of machine learning where the categories are predefined, and is used to classify unseen instances based on certain algorithm inferred from training data.\n",
    "\n",
    "<Br>\n",
    "Let's visualize our classification rule now. Mathematically, it is just a point $x=\\dfrac{-\\beta_0}{\\beta_1}$, but here we can also plot it on a 2-dimensional plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x990deb8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFRCAYAAABzDARaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQk4ZTsRUREEk7JXkREJOH+P3Podec2OqlXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x988b860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data\n",
    "hours = np.array([0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,\n",
    "                  2.75,3.00,3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50]).reshape(-1,1)\n",
    "pass_exam = np.array([0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1])\n",
    "# Plot\n",
    "x= -est[0] / est[1] # est[0], est[1] are estimated coefficients\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(hours, pass_exam)\n",
    "plt.vlines(x, -0.1, 1.1, colors='red')\n",
    "plt.title('Logistic Regression as a Linear Classifier')\n",
    "plt.xlabel('hours')\n",
    "plt.ylabel('pass_exam')\n",
    "plt.text(3.5, 0.85, r'$\\beta_0 + \\beta*x=0$', horizontalalignment='center', fontsize=16)\n",
    "plt.text(3.5, 0.2, r'$\\rightarrow$ Passed Class', horizontalalignment='center', fontsize=14)\n",
    "plt.text(1.95, 0.2, r'Failed Class $\\leftarrow$', horizontalalignment='center', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Admission into Graduate School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In example 1, there's only one independent variable (hours) and the size of the sample is quite small (only 20). To better test our code, let's web scrape a larger dataset from <http://www.ats.ucla.edu/stat/data/binary.csv>. The example is thanks to [R Data Analysis Examples: Logit Regression. UCLA: Statistical Consulting Group.](http://www.ats.ucla.edu/stat/r/dae/logit.htm). In example 2, a researcher is interested in how variables, such as GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution, effect admission into graduate school. The response variable, admit/don't admit, is a binary variable.\n",
    "\n",
    "<Br>\n",
    "We use `urllib.request` module for web scraping, and `pandas` library for reading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  rank\n",
       "0      0  380  3.61     3\n",
       "1      1  660  3.67     3\n",
       "2      1  800  4.00     1\n",
       "3      1  640  3.19     4\n",
       "4      0  520  2.93     4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "# URL\n",
    "url = 'http://www.ats.ucla.edu/stat/data/binary.csv'\n",
    "# Extract data and save into file admission.csv on current directory\n",
    "urlretrieve(url, './admission.csv')\n",
    "# Read csv data and store in DataFrame format\n",
    "data = pd.read_csv('./admission.csv')\n",
    "# Display top 5 observations\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has one categorical independent variable called rank that takes on the values 1 through 4. Let's first transform it to three dummy variables with rank=1 dropped. In Python, a package called [`pasty`](http://patsy.readthedocs.io/en/latest/index.html#) is specifically designed to build design matrices suitable for statistical models. Below, we import [`dmatrices`](http://patsy.readthedocs.io/en/latest/API-reference.html) method which returns two design matrices y and x while categorizing rank variable using method [`C()`](http://patsy.readthedocs.io/en/latest/categorical-coding.html). The number \"1\" inside the formula stands for intercept column and we also assigned argument `return_type` to `DataFrame` in `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>C(rank)[T.2]</th>\n",
       "      <th>C(rank)[T.3]</th>\n",
       "      <th>C(rank)[T.4]</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intercept  C(rank)[T.2]  C(rank)[T.3]  C(rank)[T.4]    gre   gpa\n",
       "0        1.0           0.0           1.0           0.0  380.0  3.61\n",
       "1        1.0           0.0           1.0           0.0  660.0  3.67\n",
       "2        1.0           0.0           0.0           0.0  800.0  4.00\n",
       "3        1.0           0.0           0.0           1.0  640.0  3.19\n",
       "4        1.0           0.0           0.0           1.0  520.0  2.93"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from patsy import dmatrices\n",
    "y, x = dmatrices('admit ~ 1 + gre + gpa + C(rank)', data, return_type = 'dataframe')\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply our logistic regression program to example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated coefficients: \n",
      "-3.99001808561, -0.675432176804, -1.34019999625, -1.55145348594, 0.00226443704945, 0.804044522069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ -3.99001809e+00,  -6.75432177e-01,  -1.34020000e+00,\n",
       "        -1.55145349e+00,   2.26443705e-03,   8.04044522e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y, dtype=np.float64)\n",
    "x = np.array(x, dtype=np.float64)\n",
    "logistic_regression(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our result is the same as what given on UCLA website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Scikit-learn`](http://scikit-learn.org/stable/index.html#) is a Python package that is intensely used in the field of machine learning and data science. The package implements logistic regression through class [`sklearn.linear_model.LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Before we delve into the implementation of logistic regression in `scikit-learn`, we will briefly go through several useful concepts, they are cost function, overfitting (underfitting), linear separability, L1/L2 regularization and OvR technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that when we estimated parameters using conditional maximum likelihood funciton, we minimized the negative value of likelihood function `negate(log_likelihood(x, y))`. In fact, `negate(log_likelihood(x, y))` is called cost function. As its name would suggest, the lower the cost, the better mapping from training examples to correct output.\n",
    "\n",
    "<Br />\n",
    "Since our likelihood function is $\\ln(L(x, y; \\beta))=-\\sum\\limits_{i=1}^n\\ln(1+e^{\\,-y_i\\beta'x_i})$ with redefined $y$, the cost function is therefore $cost(x, y; \\beta)=\\sum\\limits_{i=1}^n\\ln(1+e^{\\,-y_i\\beta'x_i})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, [overfitting](https://en.wikipedia.org/wiki/Overfitting) describes the situation when a model performs well on training data but has bad predictions on unseen data. It happens when a model is too complex, for instance, when a model has too many parameters and tries to even describe noisy data. Underfitting, on the other hand, happens when a model is too simple and unable to catch important signals.\n",
    "\n",
    "<Br />\n",
    "![Overfitting](http://pingax.com/wp-content/uploads/2014/05/underfitting-overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Separability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, logistic regression is a linear classifier because it eventually reduces to $\\beta'x=0$. However, when the sample classes are completely linearly separable, a problem that the maximum likelihood estimates of $\\beta$ tend to infinity would be triggered (the mathematical proof is left to the readers). In some programming languages like R, a warning message would be returned to users in this case.\n",
    "\n",
    "<br>\n",
    "<img src=\"http://3.bp.blogspot.com/-OVlHHKeYq28/Ukh6vBgVx3I/AAAAAAAACEI/5vw6LfK1xlY/s1600/linearly_separable_4.png\" width=\"300\" height=\"300\" />\n",
    "\n",
    "<br>\n",
    "There are several methods to recognize linear separability from the very beginning, and linear programming is a standard way to do this. Basically, for a two-class problem, suppose $A=\\{a_1, a_2, \\ldots, a_n\\}\\subset {\\rm I\\!R}^n$ and $B=\\{b_1, b_2, \\ldots, b_n\\}\\subset {\\rm I\\!R}^n$ are the dichotomous classes, then statement that a hyperplane such that it separates A and B exists  is equivalent to the statement that there exist a column vector $\\beta\\in {\\rm I\\!R}^n$ and a scaler $c\\in {\\rm I\\!R}$ that satisfies the inequalities as below\n",
    "\n",
    "<br>\n",
    "$$\\begin{bmatrix} a_1&-1\\\\{\\vdots}\\\\a_n&-1\\\\-b_1&1\\\\{\\vdots}\\\\-b_n&1 \\end{bmatrix}\\begin{bmatrix}\\beta\\\\c\\end{bmatrix}<\\begin{bmatrix}0\\\\ {\\vdots}\\\\0\\end{bmatrix}$$ \n",
    "\n",
    "<br>\n",
    "Feed this to any linear programming solver, for instance, [`scipy.optimize.linprog`](http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.optimize.linprog.html).\n",
    "\n",
    "<br>\n",
    "In addition, we can somehow mutate the cost function of logistic regression and get non-inflated estimates. This is exactly how `scikit-learn` tackles linear separability. It adds a regularization term to the cost function (we will talk about this soon).\n",
    "\n",
    "<br>\n",
    "At the other extreme, our training set might be anything but linearly separable, for example, in the following graph, it seems an ellipse might separate our classes.\n",
    "\n",
    "<br>\n",
    "<img src=\"http://i.stack.imgur.com/LZWS8.png=10x10\" width=\"400\" height=\"400\" />\n",
    "\n",
    "<br>\n",
    "In such case, we may need to provide a mapping function to the independent variables. For example, rather than using linear expression of independent variables as shown in $\\ln\\left(\\dfrac{p(x;\\beta)}{1-p(x;\\beta)}\\right)=\\beta'x$, we may apply the general analytical form of an ellipse such that $\\ln\\left(\\dfrac{p(x;\\beta)}{1-p(x;\\beta)}\\right)=\\beta_1{x_1^2}+\\beta_2{x_1}{x_2}+\\beta_3{x_2^2}+\\beta_4{x_1}+\\beta_5{x_2}+\\beta_6.$ In addition, we can also use nonlinear classification methods, for instance, neural network may be a good alternative here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1/L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to tackle overfitting and complete linear separability is by adding a [L1/L2 regularization](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization) term to the cost function. The idea behind regularization is to penalize parameters that have very large absolute values. \n",
    "\n",
    "<Br />\n",
    "Mathematically, L2 regularization is $\\dfrac{\\lambda}{2}\\sum\\limits_{i=1}^n\\beta_i^2$ and L1 regularization is $\\lambda\\sum\\limits_{i=1}^n|\\beta_i|$, where $\\beta_1,\\,\\beta_2,...$ are parameters to be estimated, and $\\lambda$ is a positive number called \"regularization strength\". Here, $\\lambda$ is just a weight scaler put on the regularization term, and the higher of $\\lambda$, the more weight we put on the regularization term. Also note that the denominator $2$ in the L2 term is just for the convience of computation (you can also not include them). In fact, we have $\\dfrac{\\lambda}{2}\\sum\\limits_{i=1}^n\\beta_i^2=\\dfrac{\\lambda}{2}\\beta'\\beta$, which is a quadratic form and taking derivative of this quadratic form would produce some $2$s.\n",
    "\n",
    "<Br />\n",
    "In `scikit-learn`, logistic regression minimizes \n",
    "![L2](http://scikit-learn.org/stable/_images/math/6a0bcf21baaeb0c2b879ab74fe333c0aab0d6ae6.png)\n",
    "for L1 regularity and minimizes\n",
    "![L1](http://scikit-learn.org/stable/_images/math/760c999ccbc78b72d2a91186ba55ce37f0d2cf37.png)\n",
    "for L2 regularity. They are just the sum of cost function and regularization term. To show this, take L2 regularization as an example, we have\n",
    "\n",
    "<Br />\n",
    "$$\\begin{align}\\mathrm{\\arg\\min}(\\text{L2}+cost(x, y; \\beta))&=\\mathrm{\\arg\\min}(\\dfrac{\\lambda}{2}\\beta'\\beta+\\sum\\limits_{i=1}^n\\ln(1+e^{\\,-y_i\\beta'x_i}))\\\\&=\\mathrm{\\arg\\min}(\\dfrac{1}{2}\\beta'\\beta+\\dfrac{1}{\\lambda}\\sum\\limits_{i=1}^n\\ln(1+e^{\\,-y_i\\beta'x_i}))\\\\&=\\mathrm{\\arg\\min}(\\dfrac{1}{2}\\beta'\\beta+C\\sum\\limits_{i=1}^n\\ln(1+e^{\\,-y_i\\beta'x_i}))\\end{align}$$\n",
    "\n",
    "<Br />\n",
    "Here,the capital letter $C$ in both equations are just the inverse of regularization strength, i.e., $C=\\dfrac{1}{\\lambda}$. And $y_i(X_i^Tw+c)$ is just $y_i\\beta_i'x$ in our notation (recall that we didn't explicitly write the constant coefficient for the simplicity of our likelihood function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Binary Classification to Multiclass Classification: Applying OvR Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have applied logistic regression to binary classification. But what if we have more than two classes to be classified? A `Scikit-learn`, a multiclass classification problem is by default handled by a technique called OvR (One-vs-Rest) technique. The OvR technique involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. \n",
    "\n",
    "<br>\n",
    "For example, in the following illustration, we have three classes, namely, triangles (denoted as 1), squares(denoted as 2) and crosses(denoted as 3). What we are gonna do is we can ran three times binary classification using logistic regression and get three conditional probability functions for class 1, 2 and 3. Denote them as $\\Pr\\left(y=i{\\mid}x;\\theta\\right)=h_{\\theta}^{i}(x)$, where $i=1,2,3.$ Then for each new input, we predict class $\\operatorname*{arg\\,max}_i h_{\\theta}^{i}(x).$\n",
    "\n",
    "<br>\n",
    "![Multiclass Classification](http://img.blog.csdn.net/20160218143343043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Scikit-Learn Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redo example 2 using `scikit-learn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.87278844e+00,  -6.05838296e-01,  -1.17492440e+00,\n",
       "         -1.37839850e+00,   1.81821273e-03,   2.43538568e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Set up class LogisticRegression. Note becasue we have add column 1 in x, \n",
    "# we don't need to fit intercept\n",
    "lr_model = LogisticRegression(fit_intercept=False) \n",
    "# The recommended dimension of y is 1, but our y is 2-dimensional, use \n",
    "# np.ravel to return a flattened y\n",
    "lr_model = lr_model.fit(x, np.ravel(y)) \n",
    "# Use attribute coef_ to get estimated coefficients, if fit_intercept=True,\n",
    "# further use attribute intercept_ to get constant coefficient\n",
    "lr_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the estimated parameters are different from our result. This is becasue `scikit-learn` uses regularized cost function, as we mentioned before. While there is no way to switch off the regularization term in `scikit-learn`, we can give a very large value to $C$ and therefore the weight placed on the cost function is much bigger. For this purpose, reassign argument `C` in class `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.97765654e+00,  -6.74567838e-01,  -1.33708167e+00,\n",
       "         -1.55029740e+00,   2.25513644e-03,   8.01765503e-01]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(fit_intercept=False, C=1e10) \n",
    "lr_model = lr_model.fit(x, np.ravel(y)) \n",
    "lr_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the result is the same as ours.\n",
    "\n",
    "<Br />\n",
    "Besides argument `C` (as mentioned above),`LogisticRegression` includes argument `penalty` for users choosing either L1 or L2 penalties. In terms of optimization algorithms, users can adjust by using argument `solver`. Also, class `LogisticRegression` has many useful methods, for instance, we can use `predict` to test on testing data and see if predicted class is correct. Confer [document](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to learn more.\n",
    "\n",
    "<Br />\n",
    "Let's try different settings in `LogisticRegression` and predict a student who is in rank 1, has a 700 GRE score and a 3.6 GPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(fit_intercept=False, C=2, penalty = 'l1', solver='liblinear') \n",
    "lr_model = lr_model.fit(x, np.ravel(y)) \n",
    "lr_model.coef_\n",
    "# Predicted class is 'admit'\n",
    "lr_model.predict(np.array([1,0,0,0,700,3.6]).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our prediction, this is a \"good\" student and he/she is likely to be enrolled."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
